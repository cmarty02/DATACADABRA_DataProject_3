{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsWJQbcXM06Z",
        "outputId": "3c54a248-cb4e-4991-d431-3e764d9a07e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "yJhd08W7x9Om",
        "outputId": "6c9dd964-bf8b-4393-83f7-44e7e4610238"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPbojUemYIE6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEuG8L4J1QuM"
      },
      "outputs": [],
      "source": [
        "# Load the csv file\n",
        "train = pd.read_csv('/content/drive/MyDrive/kaggel p /train.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pBOTtI5ZvQZ"
      },
      "source": [
        "# Data Exploration and Analysis and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzWcRJ_J2Y01"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FYPHFCe2Yxm"
      },
      "outputs": [],
      "source": [
        "# Display the statistical summary of the DataFrame\n",
        "print(train.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY-mLdzp70ZD"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(train.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFrIa_zLaD3L"
      },
      "source": [
        "# Processing missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fcrIrV_xbdy"
      },
      "outputs": [],
      "source": [
        "# Replace '?' and \"0\" and negative numberes  with NaN in the 'trestbps' column\n",
        "\n",
        "train['trestbps'].replace('?', np.nan, inplace=True)\n",
        "train['trestbps'].replace('0', np.nan, inplace=True)\n",
        "train['trestbps'] = train['trestbps'].apply(pd.to_numeric, errors='coerce')\n",
        "train['trestbps'][train['trestbps'] < 0] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEeOrQ0TxbU_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the average 'trestbps' for each combination of age and gender\n",
        "average_trestbps = train.groupby(['age', 'sex'])['trestbps'].mean()\n",
        "\n",
        "# Step 2: Fill in the missing values in the 'trestbps' column with the corresponding average value based on age and gender\n",
        "def fill_missing_trestbps(row):\n",
        "    if pd.isnull(row['trestbps']):\n",
        "        age = row['age']\n",
        "        sex = row['sex']\n",
        "        if (age, sex) in average_trestbps:\n",
        "            return average_trestbps[(age, sex)]\n",
        "    return row['trestbps']\n",
        "\n",
        "train['trestbps'] = train.apply(fill_missing_trestbps, axis=1)\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['trestbps'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXeU-1SaxbSP"
      },
      "outputs": [],
      "source": [
        " # Replace '?' and \"0\" and negative numberes  with NaN in the 'chol' column\n",
        "\n",
        "train['chol'].replace('?', np.nan, inplace=True)\n",
        "train['chol'].replace('0', np.nan, inplace=True)\n",
        "train['chol'] = train['chol'].apply(pd.to_numeric, errors='coerce')\n",
        "train['chol'][train['chol'] < 0] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHeO7wipxbJh"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the average 'chol' for each combination of age and gender\n",
        "average_chol = train.groupby(['age', 'sex'])['chol'].mean()\n",
        "\n",
        "# Step 2: Fill in the missing values in the 'chol' column with the corresponding average value based on age and gender\n",
        "def fill_missing_chol(row):\n",
        "    if pd.isnull(row['chol']):\n",
        "        age = row['age']\n",
        "        sex = row['sex']\n",
        "        if (age, sex) in average_chol:\n",
        "            return average_chol[(age, sex)]\n",
        "    return row['chol']\n",
        "\n",
        "train['chol'] = train.apply(fill_missing_chol, axis=1)\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['chol'].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tHHh5sdxbG_"
      },
      "outputs": [],
      "source": [
        "# Impute missing values with the mean or median\n",
        "# Let's use the median for demonstration\n",
        "chol_median = train['chol'].median()\n",
        "train['chol'].fillna(chol_median, inplace=True)\n",
        "# Check if there are any missing values left\n",
        "print(train['chol'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCWUN5nn15jM"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "print(train['fbs'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LsSGwtZ19BU"
      },
      "outputs": [],
      "source": [
        "# Replace '?' and '-9.0' with NaN in the 'fbs' column\n",
        "train['fbs'] = train['fbs'].replace(['?', '-9.0'], np.nan)\n",
        "# Convert 'fbs' column to numeric, replacing NaN with 0\n",
        "train['fbs'] = pd.to_numeric(train['fbs'], errors='coerce')\n",
        "\n",
        "# Verify the changes\n",
        "print(train['fbs'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSLw9YAK1870"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Group by 'cp' and 'age' and get the mode of 'fbs'\n",
        "mode_fbs_by_cp_age = train.groupby(['cp', 'age'])['fbs'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "\n",
        "# Step 2: Fill in missing values in 'fbs' based on the mode for the corresponding 'cp' and 'age' combination\n",
        "def fill_missing_fbs(row):\n",
        "    if pd.isnull(row['fbs']):\n",
        "        cp = row['cp']\n",
        "        age = row['age']\n",
        "        if (cp, age) in mode_fbs_by_cp_age:\n",
        "            return mode_fbs_by_cp_age[(cp, age)]\n",
        "    return row['fbs']\n",
        "\n",
        "train['fbs'] = train.apply(fill_missing_fbs, axis=1)\n",
        "\n",
        "# Verify the changes\n",
        "print(train['fbs'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzaWUEiD1848"
      },
      "outputs": [],
      "source": [
        "# Check if there are any missing values left\n",
        "print(train['fbs'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfAY6NTZ182D"
      },
      "outputs": [],
      "source": [
        "# Fill remaining missing values in 'fbs' with the mode of the entire column\n",
        "train['fbs'].fillna(train['fbs'].mode()[0], inplace=True)\n",
        "# Verify the changes\n",
        "print(train['fbs'].value_counts())\n",
        "# Check if there are any missing values left\n",
        "print(train['fbs'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk55rRKj18y6"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "print(train['restecg'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmckHRFn52Yg"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "print(train['thalach'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCjYg_5a52Vw"
      },
      "outputs": [],
      "source": [
        " # Replace '?' and \"0\" and negative numberes  with NaN in the 'thalach' column\n",
        "\n",
        "train['thalach'].replace('?', np.nan, inplace=True)\n",
        "train['thalach'].replace('0', np.nan, inplace=True)\n",
        "train['thalach'] = train['thalach'].apply(pd.to_numeric, errors='coerce')\n",
        "train['thalach'][train['thalach'] < 0] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqtLCRts52Sw"
      },
      "outputs": [],
      "source": [
        "# missing values left\n",
        "print(train['thalach'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZwEG-xW52P_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the average 'thalach' for each combination of age and gender\n",
        "average_thalach = train.groupby(['age', 'sex'])['thalach'].mean()\n",
        "\n",
        "# Step 2: Fill in the missing values in the 'thalach' column with the corresponding average value based on age and gender\n",
        "def fill_missing_thalach(row):\n",
        "    if pd.isnull(row['thalach']):\n",
        "        age = row['age']\n",
        "        sex = row['sex']\n",
        "        if (age, sex) in average_thalach:\n",
        "            return average_thalach[(age, sex)]\n",
        "    return row['thalach']\n",
        "\n",
        "train['thalach'] = train.apply(fill_missing_thalach, axis=1)\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['thalach'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWkCvEdX6u33"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "print(train['exang'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biePrJ-L6uwv"
      },
      "outputs": [],
      "source": [
        "# Replace '?' and '-9.0' with NaN in the 'exang' column\n",
        "train['exang'] = train['exang'].replace(['?'], np.nan)\n",
        "\n",
        "# Convert 'exang' column to numeric, replacing NaN with 0\n",
        "train['exang'] = pd.to_numeric(train['exang'], errors='coerce')\n",
        "\n",
        "# Verify the changes\n",
        "print(train['exang'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lInDhuHS6uoW"
      },
      "outputs": [],
      "source": [
        "# Step 1: Group by 'cp' and 'restecg' and get the mode of 'exang'\n",
        "mode_exang_by_cp_restecg = train.groupby(['cp', 'restecg'])['exang'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "\n",
        "# Step 2: Fill in missing values in 'exang' based on the mode for the corresponding 'cp' and 'restecg' combination\n",
        "def fill_missing_exang(row):\n",
        "    if pd.isnull(row['exang']):\n",
        "        cp = row['cp']\n",
        "        restecg = row['restecg']\n",
        "        if (cp, restecg) in mode_exang_by_cp_restecg:\n",
        "            return mode_exang_by_cp_restecg[(cp, restecg)]\n",
        "    return row['exang']\n",
        "\n",
        "train['exang'] = train.apply(fill_missing_exang, axis=1)\n",
        "\n",
        "# Verify the changes\n",
        "print(train['exang'].value_counts())\n",
        "# Check if there are any missing values left\n",
        "print(train['exang'].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDcYAF2k6uiu"
      },
      "outputs": [],
      "source": [
        "print(train['oldpeak'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PueSUVkQ6ufq"
      },
      "outputs": [],
      "source": [
        "# Replace '?' and '-9.0' with NaN in the 'oldpeak' column\n",
        "train['oldpeak'] = train['oldpeak'].replace(['?'], np.nan)\n",
        "# Convert 'oldpeak' column to numeric, replacing NaN with 0\n",
        "train['oldpeak'] = pd.to_numeric(train['oldpeak'], errors='coerce')\n",
        "# Verify the changes\n",
        "print(train['oldpeak'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnXOS-OF6ucn"
      },
      "outputs": [],
      "source": [
        "# Check missing values left\n",
        "print(train['oldpeak'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hWbAk7w6uZH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the average 'oldpeak' for each combination of age and gender\n",
        "average_oldpeak = train.groupby(['age', 'sex'])['oldpeak'].mean()\n",
        "\n",
        "# Step 2: Fill in the missing values in the 'oldpeak' column with the corresponding average value based on age and gender\n",
        "def fill_missing_oldpeak(row):\n",
        "    if pd.isnull(row['oldpeak']):\n",
        "        age = row['age']\n",
        "        sex = row['sex']\n",
        "        if (age, sex) in average_oldpeak:\n",
        "            return average_oldpeak[(age, sex)]\n",
        "    return row['oldpeak']\n",
        "\n",
        "train['oldpeak'] = train.apply(fill_missing_oldpeak, axis=1)\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['oldpeak'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj1LFgXW8IxO"
      },
      "outputs": [],
      "source": [
        "print(train['slope'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrjf6-cm8Iuw"
      },
      "outputs": [],
      "source": [
        "train['slope'].replace('?', np.nan, inplace=True)\n",
        "train['slope'] = train['slope'].apply(pd.to_numeric, errors='coerce')\n",
        "train['slope'][train['slope'] < 0] = np.nan\n",
        "# Convert 'slope' column to numeric, replacing NaN with 0\n",
        "train['slope'] = pd.to_numeric(train['slope'], errors='coerce')\n",
        "\n",
        "# Verify the changes\n",
        "print(train['slope'].value_counts())\n",
        "# missing values left\n",
        "print(train['slope'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2m_Mnw48Irt"
      },
      "outputs": [],
      "source": [
        "# Step 1: Group by 'cp' and 'restecg' and get the mode of 'slope'\n",
        "mode_slope_by_cp_restecg = train.groupby(['cp', 'restecg'])['slope'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "\n",
        "# Step 2: Fill in missing values in 'slope' based on the mode for the corresponding 'cp' and 'restecg' combination\n",
        "def fill_missing_slope(row):\n",
        "    if pd.isnull(row['slope']):\n",
        "        cp = row['cp']\n",
        "        restecg = row['restecg']\n",
        "        if (cp, restecg) in mode_slope_by_cp_restecg:\n",
        "            return mode_slope_by_cp_restecg[(cp, restecg)]\n",
        "    return row['slope']\n",
        "\n",
        "train['slope'] = train.apply(fill_missing_slope, axis=1)\n",
        "\n",
        "# Verify the changes\n",
        "print(train['slope'].value_counts())\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['slope'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4jMoOzT8Io1"
      },
      "outputs": [],
      "source": [
        "print(train['ca'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dc_EqYH8ImD"
      },
      "outputs": [],
      "source": [
        "train['ca'].replace('?', np.nan, inplace=True)\n",
        "train['ca'] = train['ca'].apply(pd.to_numeric, errors='coerce')\n",
        "train['ca'][train['ca'] < 0] = np.nan\n",
        "# Convert 'ca' column to numeric, replacing NaN with 0\n",
        "train['ca'] = pd.to_numeric(train['ca'], errors='coerce')\n",
        "\n",
        "# Verify the changes\n",
        "print(train['ca'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy9sdYe69M7h"
      },
      "outputs": [],
      "source": [
        "print(train['ca'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC-Tj2Ui9M4x"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the average 'ca' for each combination of age and gender\n",
        "average_ca = train.groupby(['age'])['ca'].max()\n",
        "\n",
        "# Step 2: Fill in the missing values in the 'ca' column with the corresponding average value based on age and gender\n",
        "def fill_missing_ca(row):\n",
        "    if pd.isnull(row['ca']):\n",
        "        age = row['age']\n",
        "\n",
        "        if (age) in average_ca:\n",
        "            return average_ca[(age)]\n",
        "    return row['ca']\n",
        "\n",
        "train['ca'] = train.apply(fill_missing_ca, axis=1)\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['ca'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdnPh4CI9M2I"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Step 1: Group by 'cp' and 'restecg' and get the mode of 'ca'\n",
        "mode_ca_by_cp_restecg = train.groupby(['cp', 'restecg'])['ca'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "\n",
        "# Step 2: Fill in missing values in 'ca' based on the mode for the corresponding 'cp' and 'restecg' combination\n",
        "def fill_missing_ca(row):\n",
        "    if pd.isnull(row['ca']):\n",
        "        cp = row['cp']\n",
        "        restecg = row['restecg']\n",
        "        if (cp, restecg) in mode_ca_by_cp_restecg:\n",
        "            return mode_ca_by_cp_restecg[(cp, restecg)]\n",
        "    return row['ca']\n",
        "\n",
        "train['ca'] = train.apply(fill_missing_ca, axis=1)\n",
        "\n",
        "# Verify the changes\n",
        "print(train['ca'].value_counts())\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['ca'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqC6xobD9MzG"
      },
      "outputs": [],
      "source": [
        "print(train['thal'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O-ZBtr_-OGP"
      },
      "outputs": [],
      "source": [
        "train['thal'].replace('?', np.nan, inplace=True)\n",
        "train['thal'] = train['thal'].apply(pd.to_numeric, errors='coerce')\n",
        "train['thal'][train['thal'] < 0] = np.nan\n",
        "# Convert 'ca' column to numeric, replacing NaN with 0\n",
        "train['thal'] = pd.to_numeric(train['thal'], errors='coerce')\n",
        "\n",
        "# Verify the changes\n",
        "print(train['thal'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ9tXuQQ-N_F"
      },
      "outputs": [],
      "source": [
        "print(train['thal'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2dsxRMz-N8G"
      },
      "outputs": [],
      "source": [
        "# Step 1: Group by 'cp' and 'restecg' and get the mode of 'thal'\n",
        "mode_thal_by_cp_restecg = train.groupby(['cp', 'restecg'])['thal'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "\n",
        "# Step 2: Fill in missing values in 'thal' based on the mode for the corresponding 'cp' and 'restecg' combination\n",
        "def fill_missing_thal(row):\n",
        "    if pd.isnull(row['thal']):\n",
        "        cp = row['cp']\n",
        "        restecg = row['restecg']\n",
        "        if (cp, restecg) in mode_thal_by_cp_restecg:\n",
        "            return mode_thal_by_cp_restecg[(cp, restecg)]\n",
        "    return row['thal']\n",
        "\n",
        "train['thal'] = train.apply(fill_missing_thal, axis=1)\n",
        "\n",
        "# Verify the changes\n",
        "print(train['thal'].value_counts())\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['thal'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYNP2CMH-N5W"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the average 'thal' for each combination of age and gender\n",
        "average_thal = train.groupby(['age'])['thal'].max()\n",
        "\n",
        "# Step 2: Fill in the missing values in the 'thal' column with the corresponding average value based on age and gender\n",
        "def fill_missing_thal(row):\n",
        "    if pd.isnull(row['thal']):\n",
        "        age = row['age']\n",
        "\n",
        "        if (age) in average_thal:\n",
        "            return average_thal[(age)]\n",
        "    return row['thal']\n",
        "\n",
        "train['thal'] = train.apply(fill_missing_thal, axis=1)\n",
        "\n",
        "# Check if there are any missing values left\n",
        "print(train['thal'].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D0F24Zn-N2N"
      },
      "outputs": [],
      "source": [
        "# Verify the changes\n",
        "print(train['thal'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGyxB_eTaeJ6"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhrSR7BH-qQQ"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfMuuiGwsW3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "train['age'].plot(kind='line', figsize=(8, 4), title='age')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bv3IT3ksR_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "train['trestbps'].plot(kind='line', figsize=(8, 4), title='trestbps')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAahxGMesJe6"
      },
      "outputs": [],
      "source": [
        "\n",
        "train.plot(kind='scatter', x='trestbps', y='chol', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXeGrDnJBXwd"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of the target variable\n",
        "sns.countplot(x='label', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1_khaYnBVa1"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the sex distribution\n",
        "sns.countplot(x='sex', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yJz7o8x_lha"
      },
      "outputs": [],
      "source": [
        "# prompt: age info\n",
        "\n",
        "# Display the distribution of the age variable\n",
        "sns.distplot(train['age'].dropna())\n",
        "plt.show()\n",
        "\n",
        "# Print the minimum, maximum, mean, and median of the age variable\n",
        "print('Minimum age:', train['age'].min())\n",
        "print('Maximum age:', train['age'].max())\n",
        "print('Mean age:', train['age'].mean())\n",
        "print('Median age:', train['age'].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vszSdvVn-qMl"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the exang distribution\n",
        "sns.countplot(x='ca', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClGVKcAk-qJz"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the exang distribution\n",
        "sns.countplot(x='thal', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GWSYDRGAg14"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the exang distribution\n",
        "sns.countplot(x='label', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GOtuQDtAgzu"
      },
      "outputs": [],
      "source": [
        "# Print the distribution of the 'oldpeak' column after handling missing values\n",
        "sns.histplot(train['oldpeak'].dropna(), kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmqqMCxvAgwk"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the exang distribution\n",
        "sns.countplot(x='exang', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V_woeXiAgt_"
      },
      "outputs": [],
      "source": [
        "sns.distplot(train['thalach'].dropna())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnsYjkk-Agq5"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the restecg distribution\n",
        "sns.countplot(x='restecg', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnYkbAPF-qGt"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the fbs distribution\n",
        "sns.countplot(x='fbs', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaOVg7mjA-_J"
      },
      "outputs": [],
      "source": [
        "# Print the distribution of the 'trestbps' column after handling missing values\n",
        "sns.histplot(train['chol'].dropna(), kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKGxLKjaA-8d"
      },
      "outputs": [],
      "source": [
        " # Print the distribution of the 'trestbps' column after handling missing values\n",
        "\n",
        "sns.distplot(train['trestbps'].dropna())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bsibhn9A-5n"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart of the sex distribution\n",
        "sns.countplot(x='cp', data=train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cy-LFCpA-2k"
      },
      "outputs": [],
      "source": [
        "# Display the statistical summary of the DataFrame\n",
        "print(train.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Tw01JcBatN"
      },
      "outputs": [],
      "source": [
        "train.info()\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtoW0SZUBaqM"
      },
      "outputs": [],
      "source": [
        "corr = train.corr()\n",
        "corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsle0SE5Bano"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sIP2eZpgrak"
      },
      "source": [
        "# first model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzU4B8ZHfyFU"
      },
      "source": [
        "\n",
        "**PCA to Speed up Machine Learning Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaArhbVdhWns"
      },
      "source": [
        "**Prepare Dataset for Machine Learning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNojkG32ffkP"
      },
      "source": [
        "**usuccessful attempt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmuqORY-e_JP"
      },
      "source": [
        "**Training accuracy: 0.5591985428051002\n",
        "Testing accuracy: 0.5300546448087432**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jpoezpsBafN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save X data\n",
        "X = train.drop(columns = 'label')\n",
        "# Encode our target\n",
        "y = train['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFtsPWcABace"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3QpM2JKfVpn"
      },
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBbG5DD5hpkM"
      },
      "source": [
        "**Scale Data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsDbH-6cBaaH"
      },
      "outputs": [],
      "source": [
        "# Instantiate Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "# Fit & transform data.\n",
        "scaled_df = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hgQEd0Ot7vS"
      },
      "outputs": [],
      "source": [
        "# Instantiate, fit & transform data using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(scaled_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXk6MZoRiByq"
      },
      "source": [
        "**Choosing the Number of Components to Return**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL-fCX1HivXl"
      },
      "source": [
        "**PCA for  Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ceBTxKJuEWa"
      },
      "outputs": [],
      "source": [
        "# Visualize the first 2 PCs\n",
        "plt.figure(figsize = (8, 4))\n",
        "plt.scatter(pcs[:,0], pcs[:,1], c = y)\n",
        "plt.title('Visualization of all of our data using the first two Principal Components')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVH0aY9SxNpv"
      },
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "pca.fit(scaled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BppSEUzIGiH1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bmcR6jfi7yK"
      },
      "source": [
        "*Split Data into Training & Testing Sets*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d465fhb4e4N4"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEOhBLegjpiB"
      },
      "source": [
        "Create a PCA Pipeline with Standard Scaler\n",
        "PCA works great in a pipeline to simplify our code and prevent data leakage. Remember, we always scale data before applying PCA!\n",
        "\n",
        "The scree plot above seemed to level off at about 7 components. That's how many we will use in our PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZidtdHeefs3"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eWywb-aeyNX"
      },
      "outputs": [],
      "source": [
        "# Instantiate the transformers\n",
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components = 7)\n",
        "\n",
        "# create a transformer pipe\n",
        "transformer = make_pipeline(scaler, pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzD8LXXXyBby"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqcVkt90jyIb"
      },
      "source": [
        "Put the PCA Pipeline in Another Pipeline with a Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FOcZTtmyQWc"
      },
      "outputs": [],
      "source": [
        "# Create a transformer pipeline\n",
        "transformer = make_pipeline(StandardScaler(), PCA(n_components=7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaiCSZj5eyKP"
      },
      "outputs": [],
      "source": [
        "# Instantiate logistic regression\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Create a modeling pipeline\n",
        "logreg_pipe = make_pipeline(transformer, logreg)\n",
        "logreg_pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq36tQQ-j-AL"
      },
      "source": [
        "Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xng2cCyPeyHh"
      },
      "outputs": [],
      "source": [
        "print('Training accuracy:', logreg_pipe.score(X_train, y_train))\n",
        "print('Testing accuracy:', logreg_pipe.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbV_jWBthHD3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make predictions on the training and testing datasets\n",
        "train_predictions = logreg_pipe.predict(X_train)\n",
        "test_predictions = logreg_pipe.predict(X_test)\n",
        "\n",
        "# Calculate F1-score for the training and testing datasets\n",
        "train_f1_score = f1_score(y_train, train_predictions, average='weighted')\n",
        "test_f1_score = f1_score(y_test, test_predictions, average='weighted')\n",
        "\n",
        "print('Training F1-score:', train_f1_score)\n",
        "print('Testing F1-score:', test_f1_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcdv_IiXnMrC"
      },
      "source": [
        "# second models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eoyzAesnIUB"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC4fVV2wfbdv"
      },
      "source": [
        "**usuccessful attempt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-XlrBx8fUjH"
      },
      "source": [
        "**F1 Score: 0.45464860912015376**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1V6lx9KgfzN"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into features and target variable\n",
        "X = train.drop(columns=['label'])\n",
        "y = train['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEmI7E56lwLe"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYazvnFulxSw"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z5wnWEhlze3"
      },
      "outputs": [],
      "source": [
        "# Model building and training\n",
        "# Example 1: Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKCZSqGDl1Cu"
      },
      "outputs": [],
      "source": [
        "# Example 2: Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm4QBKLRl292"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "# Example 1: Logistic Regression\n",
        "y_pred_logreg = logreg.predict(X_test_scaled)\n",
        "f1_logreg = f1_score(y_test, y_pred_logreg, average='weighted')\n",
        "print(\"Logistic Regression F1 Score:\", f1_logreg)\n",
        "print(\"Classification Report (Logistic Regression):\\n\", classification_report(y_test, y_pred_logreg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gelhggFbl5FC"
      },
      "outputs": [],
      "source": [
        "# Example 2: Random Forest Classifier\n",
        "y_pred_rf = rf_classifier.predict(X_test_scaled)\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "print(\"Random Forest Classifier F1 Score:\", f1_rf)\n",
        "print(\"Classification Report (Random Forest Classifier):\\n\", classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2CWoDQfl7Rx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Perform cross-validation on the training data\n",
        "cv_scores_logreg = cross_val_score(logreg, X_train_scaled, y_train, cv=5, scoring='f1_weighted')\n",
        "cv_scores_rf = cross_val_score(rf_classifier, X_train_scaled, y_train, cv=5, scoring='f1_weighted')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation F1 Scores (Logistic Regression):\", cv_scores_logreg)\n",
        "print(\"Cross-Validation F1 Scores (Random Forest Classifier):\", cv_scores_rf)\n",
        "\n",
        "# Inspect the distribution of the target variable\n",
        "print(\"Training Set Distribution of Target Variable (thal):\")\n",
        "print(y_train.value_counts())\n",
        "print(\"Test Set Distribution of Target Variable (thal):\")\n",
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE1HCMzRUwGm"
      },
      "source": [
        "# Overall, this approach allows you to search for the best hyperparameters for RandomForestClassifier and AdaBoostClassifier using cross-validation and evaluate their performance on unseen data using the F1-score metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbrx8Lqiobpq"
      },
      "source": [
        "# the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX3zBzz5oNsr"
      },
      "source": [
        "\n",
        "**AdaBoost - Testing F1-score: 1.0**\n",
        "\n",
        "**AdaBoost - Best F1-score: 0.9405564817019926**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IJxu4I8hxH1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH6mOibKaKYo"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuhQJUXuaVv2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Saiz5tdeuT47"
      },
      "outputs": [],
      "source": [
        "# Define features (X) and target variable (y)\n",
        "X = train.drop('label', axis=1)  # Features\n",
        "y = train['label']  # Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O38SpEuNjewI"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "# Load the dataset\n",
        "#data = load_iris()\n",
        "#X, y = data.data, data.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a custom scoring function for F1-score\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Define a pipeline for Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'rf__n_estimators': [100, 150, 200, 250, 300],\n",
        "    'rf__max_depth': [None, 5, 10, 15, 20],\n",
        "    'rf__min_samples_split': [2, 4, 6, 8, 10],\n",
        "    'rf__min_samples_leaf': [1, 2, 3, 4, 5],\n",
        "    'rf__max_features': ['sqrt', 'log2', None, 0.5, 0.8]\n",
        "}\n",
        "\n",
        "# Define a pipeline for AdaBoost\n",
        "ada_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ada', AdaBoostClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid for AdaBoost\n",
        "ada_param_grid = {\n",
        "    'ada__n_estimators': [50, 100, 150, 200, 250],\n",
        "    'ada__learning_rate': [0.1, 0.5, 1.0, 1.5, 2.0]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for Random Forest\n",
        "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, scoring=f1_scorer, cv=5, n_jobs=-1)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Perform GridSearchCV for AdaBoost\n",
        "ada_grid_search = GridSearchCV(ada_pipeline, ada_param_grid, scoring=f1_scorer, cv=5, n_jobs=-1)\n",
        "ada_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and F1-scores\n",
        "print(\"Random Forest - Best Parameters:\", rf_grid_search.best_params_)\n",
        "print(\"Random Forest - Best F1-score:\", rf_grid_search.best_score_)\n",
        "print(\"Random Forest - Testing F1-score:\", f1_score(y_test, rf_grid_search.predict(X_test), average='weighted'))\n",
        "\n",
        "print(\"AdaBoost - Best Parameters:\", ada_grid_search.best_params_)\n",
        "print(\"AdaBoost - Best F1-score:\", ada_grid_search.best_score_)\n",
        "print(\"AdaBoost - Testing F1-score:\", f1_score(y_test, ada_grid_search.predict(X_test), average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgpPEdRX5m2T"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the iris dataset\n",
        "#data = load_iris()\n",
        "#X, y = data.data, data.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a custom scoring function for F1-score\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Define pipelines and parameter grids for each classifier\n",
        "classifiers = {\n",
        "    'Random Forest': (RandomForestClassifier(random_state=42), {\n",
        "        'classifier__n_estimators': [100, 200, 300],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4],\n",
        "        'classifier__max_features': ['sqrt', 'log2']\n",
        "    }),\n",
        "    'AdaBoost': (AdaBoostClassifier(random_state=42), {\n",
        "        'classifier__n_estimators': [50, 100, 200],\n",
        "        'classifier__learning_rate': [0.1, 0.5, 1.0]\n",
        "    }),\n",
        "    'SVM': (SVC(random_state=42), {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    }),\n",
        "    'Logistic Regression': (LogisticRegression(random_state=42), {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__penalty': ['l1', 'l2']\n",
        "    })\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for each classifier\n",
        "for name, (classifier, param_grid) in classifiers.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, scoring=f1_scorer, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Print best parameters and F1-scores\n",
        "    print(f\"{name} - Best Parameters:\", grid_search.best_params_)\n",
        "    print(f\"{name} - Best F1-score:\", grid_search.best_score_)\n",
        "    print(f\"{name} - Testing F1-score:\", f1_score(y_test, grid_search.predict(X_test), average='weighted'))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0uITVt0Auru"
      },
      "outputs": [],
      "source": [
        "#data = load_iris()\n",
        "#X, y = data.data, data.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a custom scoring function for F1-score\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Define a pipeline for Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 300],\n",
        "    'rf__max_depth': [None, 10, 20],\n",
        "    'rf__min_samples_split': [2, 5, 10],\n",
        "    'rf__min_samples_leaf': [1, 2, 4],\n",
        "    'rf__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Define a pipeline for AdaBoost\n",
        "ada_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ada', AdaBoostClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid for AdaBoost\n",
        "ada_param_grid = {\n",
        "    'ada__n_estimators': [50, 100, 200],\n",
        "    'ada__learning_rate': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for Random Forest\n",
        "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, scoring=f1_scorer, cv=5, n_jobs=-1)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Perform GridSearchCV for AdaBoost\n",
        "ada_grid_search = GridSearchCV(ada_pipeline, ada_param_grid, scoring=f1_scorer, cv=5, n_jobs=-1)\n",
        "ada_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and F1-scores\n",
        "print(\"Random Forest - Best Parameters:\", rf_grid_search.best_params_)\n",
        "print(\"Random Forest - Best F1-score:\", rf_grid_search.best_score_)\n",
        "print(\"Random Forest - Testing F1-score:\", f1_score(y_test, rf_grid_search.predict(X_test), average='weighted'))\n",
        "\n",
        "print(\"AdaBoost - Best Parameters:\", ada_grid_search.best_params_)\n",
        "print(\"AdaBoost - Best F1-score:\", ada_grid_search.best_score_)\n",
        "print(\"AdaBoost - Testing F1-score:\", f1_score(y_test, ada_grid_search.predict(X_test), average='weighted'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQRUn09IatEx"
      },
      "source": [
        "# Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePa0EvJOCpQt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Perform cross-validation for Random Forest\n",
        "rf_cv_scores = cross_val_score(rf_grid_search.best_estimator_, X_train, y_train, cv=5, scoring=f1_scorer)\n",
        "print(\"Random Forest - Cross-Validation F1 Scores:\", rf_cv_scores)\n",
        "print(\"Random Forest - Mean Cross-Validation F1 Score:\", rf_cv_scores.mean())\n",
        "\n",
        "# Perform cross-validation for AdaBoost\n",
        "ada_cv_scores = cross_val_score(ada_grid_search.best_estimator_, X_train, y_train, cv=5, scoring=f1_scorer)\n",
        "print(\"AdaBoost - Cross-Validation F1 Scores:\", ada_cv_scores)\n",
        "print(\"AdaBoost - Mean Cross-Validation F1 Score:\", ada_cv_scores.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e9eFOdGQYet"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate on test set\n",
        "y_pred = ada_grid_search.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve and AUC score (for binary classification)\n",
        "if len(train['label'].unique()) == 2:\n",
        "    y_prob = ada_grid_search.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc_score(y_test, y_prob):.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Feature Importance (if applicable)\n",
        "if hasattr(ada_grid_search.best_estimator_, 'feature_importances_'):\n",
        "    feature_importances = ada_grid_search.best_estimator_.feature_importances_\n",
        "    sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "    features = X.columns\n",
        "    top_n = 10\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=feature_importances[sorted_idx][:top_n], y=features[sorted_idx][:top_n])\n",
        "    plt.xlabel(\"Feature Importance\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.title(\"Top 10 Feature Importances\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWu5zZ0ChDBm"
      },
      "outputs": [],
      "source": [
        "X.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr0Kxne9p4eu"
      },
      "source": [
        "**mouhamedkrimi12@gmail.com**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCChfl10t6ER"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeMMFxfJt6BI"
      },
      "outputs": [],
      "source": [
        "# Assuming 'test' DataFrame contains the preprocessed test data\n",
        "\n",
        "# Make predictions using the Random Forest model\n",
        "rf_predictions = rf_grid_search.predict(test)\n",
        "\n",
        "# Make predictions using the AdaBoost model\n",
        "ada_predictions = ada_grid_search.predict(test)\n",
        "\n",
        "# Add the predictions to the test DataFrame\n",
        "test['rf_prediction'] = rf_predictions\n",
        "test['ada_prediction'] = ada_predictions\n",
        "\n",
        "# Print predictions\n",
        "print(\"Random Forest Predictions:\")\n",
        "print(test[['id', 'rf_prediction']])\n",
        "\n",
        "print(\"\\nAdaBoost Predictions:\")\n",
        "print(test[['id', 'ada_prediction']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzWX3Ftit5-p"
      },
      "outputs": [],
      "source": [
        "# Define the path for the CSV file in Google Drive\n",
        "path = \"final_results.csv\"\n",
        "\n",
        "# Save the filtered data as a CSV file\n",
        "test.to_csv(path)\n",
        "\n",
        "print(\"Data has been saved to :\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OAI-YQZt57Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
